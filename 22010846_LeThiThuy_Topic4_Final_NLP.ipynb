{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaVoi52Hz/NLP/blob/main/22010846_LeThiThuy_Topic4_Final_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project Final NLP (Natural Language Processing)**\n",
        "##***Topic: Detecting Stress and Anxiety Indicators Using Deep Learning-Based Mental Health Text Classification***\n",
        "\n",
        "**Full Name:** ***Le Thi Thuy***\n",
        "\n",
        "**Student ID: *22010846***\n",
        "\n",
        "**SCHOOL OF ELECTRICAL AND ELECTRONIC ENGINEERING - PHENIKAA UNIVERSITY**\n",
        "\n",
        "***NATURAL LANGUAGE PROCESSING COURSE***\n",
        "\n",
        "*Robotics and Artificial Intelligence Engineering Class*\n",
        "\n",
        "*Cohort: 16*\n",
        "\n",
        "*Email: 22010846@st.phenikaa-uni.edu.vn*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZ2tvTf7OjJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1: Installing Libraries and Splitting the Dataset**\n",
        "## **1.1. Installing the Necessary Libraries**\n"
      ],
      "metadata": {
        "id": "uhIHkQKZw1Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Phần 1: Cài Đặt Thư Viện Và Chia Tập Dữ Liệu**\n",
        "## **1.1. Cài đặt các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "tDrzdPFFP9hW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84NMt15XwnuQ"
      },
      "outputs": [],
      "source": [
        "# Cài đặt thư viện xử lý tiếng Việt và mô hình Transformer\n",
        "!pip install pyvi underthesea transformers datasets\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Data Preprocessing and Statistics**"
      ],
      "metadata": {
        "id": "kzBbrObAPx3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.2. Tiền xử lý và Thống kê dữ liệu**"
      ],
      "metadata": {
        "id": "qEXOee2BxBnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "df = pd.read_csv('DataFinalNLP.csv', encoding='utf-8')\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# 2. Hàm làm sạch văn bản chuyên sâu\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    # Xóa các ký tự đặc biệt, chỉ giữ lại chữ cái và số\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = word_tokenize(text, format=\"text\")\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['Text_clear'].apply(clean_text)\n",
        "\n",
        "# 3. Thống kê tập dữ liệu\n",
        "print(\"\\nDATASET STATISTICS\")\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(\"\\nSamples per class:\")\n",
        "print(df['Final_Label'].value_counts())\n",
        "\n",
        "df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
        "print(f\"\\nAverage text length: {df['word_count'].mean():.2f} words\")\n",
        "print(f\"Median text length: {df['word_count'].median()} words\")\n",
        "\n",
        "# Kiểm tra tỷ lệ mất cân bằng lớp\n",
        "imbalance_ratio = df['Final_Label'].value_counts(normalize=True) * 100\n",
        "print(\"\\nClass distribution (%):\")\n",
        "print(imbalance_ratio)"
      ],
      "metadata": {
        "id": "sGFC20StxTmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3. Split the dataset (80/10/10)**"
      ],
      "metadata": {
        "id": "D5hH0056QFQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3. Chia tập dữ liệu (80/10/10)**"
      ],
      "metadata": {
        "id": "S0MJ302IL2se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['Text_clear'],\n",
        "    df['Final_Label'],\n",
        "    test_size=0.2,\n",
        "    stratify=df['Final_Label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Chia 20% còn lại thành Val (10%) và Test (10%)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Train samples: {len(train_texts)} (80%)\")\n",
        "print(f\"Val samples:   {len(val_texts)} (10%)\")\n",
        "print(f\"Test samples:  {len(test_texts)} (10%)\")"
      ],
      "metadata": {
        "id": "4IAHxdxGNK4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: Architectural Design, Modeling, and Testing**\n",
        "\n",
        "##**2.1: KimCNN Architectural Design**\n",
        "\n",
        "This architecture consists of three main blocks:\n",
        "\n",
        "+ Block 1: Embedding Layer: Transforms each word into a 100-dimensional vector so that the machine can understand its semantics.\n",
        "\n",
        "+ Block 2: Multi-filter Convolutional Layer: Designed three filter sizes: 3, 4, and 5. Filter 3 will capture phrases like \"very tired,\" and filter 5 will capture longer phrases like \"don't want to do anything.\"\n",
        "\n",
        "+ Block 3: Max-over-time Pooling: Only retains the strongest features from the filters to include in the final classification layer."
      ],
      "metadata": {
        "id": "rHS8hPpCQS2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Phần 2: Thiết Kế Kiến Trúc Mô Hình Và Kiểm Thử**\n",
        "\n",
        "##**2.1: Thiết kế kiến trúc KimCNN**\n",
        "\n",
        "Kiến trúc này gồm 3 khối chính:\n",
        "\n",
        "+ Khối 1: Embedding Layer: Biến mỗi từ thành một vector 100 chiều để máy hiểu được ngữ nghĩa.\n",
        "\n",
        "+ Khối 2: Multi-filter Convolutional Layer: Thiết kế 3 kích thước filter là 3, 4, 5. Filter 3 sẽ bắt các cụm như \"rất mệt mỏi\", filter 5 sẽ bắt các cụm dài hơn như \"không muốn làm gì cả\".\n",
        "\n",
        "+ Khối 3: Max-over-time Pooling: Chỉ giữ lại đặc trưng mạnh nhất từ các filter để đưa vào lớp phân loại cuối cùng."
      ],
      "metadata": {
        "id": "89jZTGtRN6iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KimCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        # Lớp nhúng: Biến từ thành vector 100 chiều\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=n_filters,\n",
        "                      kernel_size=(fs, embed_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sent_len]\n",
        "        embedded = self.embedding(text).unsqueeze(1)\n",
        "\n",
        "        # Qua Conv và ReLU (hàm kích hoạt)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # Max-pooling: Lấy đặc trưng nổi bật nhất\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        # Kết hợp các đặc trưng lại và qua Dropout\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)"
      ],
      "metadata": {
        "id": "5Y9U6erCOhtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model with parameters (Hyperparameters)."
      ],
      "metadata": {
        "id": "P2YOUa1BRTWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khởi tạo mô hình với các tham số (Hyperparameters)"
      ],
      "metadata": {
        "id": "zKli1UpuRPDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thông số mô hình\n",
        "VOCAB_SIZE = 5000\n",
        "EMBED_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 3\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = KimCNN(VOCAB_SIZE, EMBED_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "# Tính trọng số dựa trên bảng thống kê 1940 mẫu\n",
        "# Nhãn 0: 571, Nhãn 1: 653, Nhãn 2: 716\n",
        "weights = torch.tensor([1940/571, 1940/653, 1940/716], dtype=torch.float)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "NLEUqc-xRUp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1 Training loop**"
      ],
      "metadata": {
        "id": "73sEcUhhS5YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(texts, max_vocab_size=5000):\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        all_words.extend(str(text).split())\n",
        "\n",
        "    counts = Counter(all_words)\n",
        "    # Gán chỉ số: 0 cho PAD, 1 cho UNK, còn lại bắt đầu từ 2\n",
        "    vocab = {word: i+2 for i, (word, _) in enumerate(counts.most_common(max_vocab_size))}\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "# 2. Khởi tạo biến vocab\n",
        "vocab = build_vocab(train_texts)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ],
      "metadata": {
        "id": "eEbY-ZcpVrR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 1. Chuyển văn bản thành số và Padding\n",
        "def encode_text(text, vocab, max_len=64):\n",
        "    tokenized = str(text).split()\n",
        "    encoded = [vocab.get(word, 1) for word in tokenized] # 1 là <UNK>\n",
        "    if len(encoded) < max_len:\n",
        "        encoded += [0] * (max_len - len(encoded)) # 0 là <PAD>\n",
        "    return encoded[:max_len]\n",
        "\n",
        "class MentalHealthDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = [torch.tensor(encode_text(t, vocab)) for t in texts]\n",
        "        self.labels = torch.tensor(labels.values)\n",
        "\n",
        "    def __len__(self): return len(self.labels)\n",
        "    def __getitem__(self, idx): return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# 2. Khởi tạo Loader thực tế\n",
        "train_dataset = MentalHealthDataset(train_texts, train_labels, vocab)\n",
        "val_dataset = MentalHealthDataset(val_texts, val_labels, vocab)\n",
        "test_dataset = MentalHealthDataset(test_texts, test_labels, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "print(f\"DataLoader has been created {len(train_dataset)} Train exercise template!\")"
      ],
      "metadata": {
        "id": "p_S2CqeDTWrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "history_loss = []\n",
        "history_f1 = []\n",
        "\n",
        "def calculate_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs = model(texts)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "EPOCHS = 20\n",
        "print(f\"Start training KimCNN on {device}...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Giai đoạn Train\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Giai đoạn Evaluate (Đánh giá trên tập Validation)\n",
        "    val_acc, val_f1 = calculate_metrics(model, val_loader)\n",
        "\n",
        "    history_loss.append(total_loss/len(train_loader))\n",
        "    history_f1.append(val_f1)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # In kết quả\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {total_loss/len(train_loader):.4f} | '\n",
        "          f'Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | '\n",
        "          f'Time: {int(end_time - start_time)}s')\n",
        "\n"
      ],
      "metadata": {
        "id": "G8ngEdbRSniv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2. Draw a graph of learning progress.**"
      ],
      "metadata": {
        "id": "QDeLmxHrZjel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Vẽ đồ thị Loss (Tự động lấy dữ liệu từ lần chạy vừa xong)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(history_loss)+1), history_loss, 'r-o', label='Train Loss')\n",
        "plt.title('Loss Curve - KimCNN')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Vẽ đồ thị F1-Score\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(history_f1)+1), history_f1, 'b-s', label='Val Macro F1')\n",
        "plt.title('F1-Score Curve - KimCNN')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RuMPjbFSY59I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.3. Confusion Matrix**"
      ],
      "metadata": {
        "id": "bOqWlkLrXZSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Lấy dự đoán từ tập Test\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix KimCNN')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6_orVLh2XeVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.4. Generate detailed reports by class.**"
      ],
      "metadata": {
        "id": "v-f_XrFNZXvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "model.eval()\n",
        "y_pred_1, y_true_1 = [], []\n",
        "\n",
        "start_inf = time.time()\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        y_pred_1.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_1.extend(labels.numpy())\n",
        "end_inf = time.time()\n",
        "\n",
        "# Tính toán các chỉ số\n",
        "inf_time_1 = (end_inf - start_inf) / len(test_loader.dataset)\n",
        "acc_1 = accuracy_score(y_true_1, y_pred_1)\n",
        "macro_f1_1 = f1_score(y_true_1, y_pred_1, average='macro')\n",
        "weighted_f1_1 = f1_score(y_true_1, y_pred_1, average='weighted')\n",
        "\n",
        "print(f\"MODEL 1 STATISTICS: KIMCNN\")\n",
        "print(f\"1. Accuracy: {acc_1*100:.2f}%\")\n",
        "print(f\"2. Macro-F1: {macro_f1_1:.4f}\")\n",
        "print(f\"3. Weighted-F1: {weighted_f1_1:.4f}\")\n",
        "print(f\"4. Avg Inference Time: {inf_time_1*1000:.4f} ms/sample\")\n",
        "print(f\"\\n5. Per-class Detail\")\n",
        "print(classification_report(y_true_1, y_pred_1, target_names=['Stress', 'Anxiety', 'Normal']))"
      ],
      "metadata": {
        "id": "Ic-ptUEPZGe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Tạo ma trận nhầm lẫn\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "# Chuẩn hóa theo hàng (chia cho tổng mỗi hàng)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix (Normalized by Row) - KimCNN')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSuJ5J0Pella"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.5. Evaluating the model test**"
      ],
      "metadata": {
        "id": "Qwt7XG0Qnqvh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eboJqytc4MS"
      },
      "source": [
        "import random\n",
        "\n",
        "def test_random_samples(model, vocab, test_texts, test_labels, num_samples=5):\n",
        "    model.eval()\n",
        "    classes = ['Stress', 'Anxiety', 'Normal']\n",
        "\n",
        "    # Tạo danh sách các chỉ số ngẫu nhiên từ tập Test\n",
        "    random_indices = random.sample(range(len(test_texts)), num_samples)\n",
        "\n",
        "    print(f\" EVALUATING {num_samples} RANDOM SAMPLES FROM TEST SET \")\n",
        "\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        text = test_texts.iloc[idx]\n",
        "        true_label = test_labels.iloc[idx]\n",
        "\n",
        "        # Tiền xử lý và dự đoán\n",
        "        tokens = encode_text(text, vocab)\n",
        "        tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tensor)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        print(f\"\\n[{i+1}] Index: {idx} | Text: {text}\")\n",
        "        print(f\"   Predicted: {classes[pred_label]} ({probs[0][pred_label].item()*100:.2f}%) | Actual: {classes[true_label]}\")\n",
        "\n",
        "        # Đánh giá nhanh đúng/sai\n",
        "        status = \"CORRECT\" if pred_label == true_label else \"INCORRECT\"\n",
        "        print(f\"   Result: {status}\")\n",
        "\n",
        "# Chạy thử nghiệm ngẫu nhiên\n",
        "test_random_samples(model, vocab, test_texts, test_labels, num_samples=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.2. BiLSTM + Attention Architectural Design**"
      ],
      "metadata": {
        "id": "uxpV20bhj7ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.2. Thiết kế kiến trúc BiLSTM + Attention**"
      ],
      "metadata": {
        "id": "jVL_Kbl8jwve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MentalHealth_BiLSTM_Att(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout):\n",
        "        super(MentalHealth_BiLSTM_Att, self).__init__()\n",
        "        # 1. Lớp Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 2. Lớp BiLSTM: bidirectional=True để học từ cả 2 chiều\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=2,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout)\n",
        "\n",
        "        # 3. Cơ chế Self-Attention\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # 4. Lớp Fully Connected\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch size, sent len]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        # lstm_out: [batch size, sent len, hid dim * 2]\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Tính trọng số Attention: xác định từ nào quan trọng nhất trong câu\n",
        "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "\n",
        "        # Tổng hợp ngữ cảnh dựa trên trọng số Attention\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "        return self.fc(self.dropout(context))\n",
        "\n"
      ],
      "metadata": {
        "id": "47IrMRb-kGjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.1. Initialize and Set Up Hyperparameters**"
      ],
      "metadata": {
        "id": "62HpnrI5kQeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình tham số\n",
        "HIDDEN_DIM = 128\n",
        "EMBED_DIM = 100\n",
        "DROPOUT = 0.5\n",
        "VOCAB_SIZE = len(vocab)\n",
        "OUTPUT_DIM = 3 # Stress, Anxiety, Normal\n",
        "\n",
        "model_2 = MentalHealth_BiLSTM_Att(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
        "\n",
        "# Sử dụng Adam Optimizer và CrossEntropy có trọng số để xử lý mất cân bằng dữ liệu\n",
        "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=1e-3)\n",
        "criterion_2 = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "\n",
        "# List lưu lịch sử tự động cho mô hình 2\n",
        "history_loss_2 = []\n",
        "history_f1_2 = []"
      ],
      "metadata": {
        "id": "8K0dbrUVkL9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.2. Training Loop**"
      ],
      "metadata": {
        "id": "e3s--mYyk3Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(20):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Giai đoạn Train\n",
        "    model_2.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer_2.zero_grad()\n",
        "        outputs = model_2(texts)\n",
        "        loss = criterion_2(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_2.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Giai đoạn Đánh giá\n",
        "    val_acc, val_f1 = calculate_metrics(model_2, val_loader)\n",
        "\n",
        "    # Lưu lịch sử\n",
        "    history_loss_2.append(total_loss/len(train_loader))\n",
        "    history_f1_2.append(val_f1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {history_loss_2[-1]:.4f} | Val F1: {val_f1:.4f} | Time: {int(end_time - start_time)}s')\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7lnCVXdk9rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.3. Export Metrics**"
      ],
      "metadata": {
        "id": "moWCNtxRlpAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import time\n",
        "\n",
        "model_2.eval()\n",
        "y_pred_2, y_true_2 = [], []\n",
        "\n",
        "# Đo thời gian inference\n",
        "start_inf = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model_2(texts)\n",
        "        y_pred_2.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_2.extend(labels.numpy())\n",
        "\n",
        "end_inf = time.time()\n",
        "inference_time = (end_inf - start_inf) / len(test_loader.dataset)\n",
        "\n",
        "# Tính các chỉ số bắt buộc\n",
        "acc_2 = accuracy_score(y_true_2, y_pred_2)\n",
        "macro_f1_2 = f1_score(y_true_2, y_pred_2, average='macro')\n",
        "weighted_f1_2 = f1_score(y_true_2, y_pred_2, average='weighted')\n",
        "\n",
        "print(f\"BILSTM + ATTENTION EXPERIMENTAL RESULTS\")\n",
        "print(f\"1. Accuracy: {acc_2*100:.2f}%\")\n",
        "print(f\"2. Macro-F1: {macro_f1_2:.4f}\")\n",
        "print(f\"3. Weighted-F1: {weighted_f1_2:.4f}\")\n",
        "print(f\"4. Average Inference Time: {inference_time*1000:.4f} ms/sample\")\n",
        "print(f\"5. Per-class Detail:\\n\")\n",
        "print(classification_report(y_true_2, y_pred_2, target_names=['Stress', 'Anxiety', 'Normal']))"
      ],
      "metadata": {
        "id": "Xzp1FKSSlpal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.4. Confusion Matrix**"
      ],
      "metadata": {
        "id": "3ilNVdL3mP-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "cm_2 = confusion_matrix(y_true_2, y_pred_2)\n",
        "cm_norm_2 = cm_2.astype('float') / cm_2.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_norm_2, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix (Normalized) - BiLSTM + Attention')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Labe')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G7N4gR-LmMdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.5. Graph comparing learning progress**"
      ],
      "metadata": {
        "id": "PjYxqhu7r24N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Đồ thị F1-Score\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_f1, label='KimCNN (Baseline)')\n",
        "plt.plot(history_f1_2, label='BiLSTM + Att (Ours)')\n",
        "plt.title('Compare Val F1-Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1')\n",
        "plt.legend()\n",
        "\n",
        "# Đồ thị Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_loss, label='KimCNN Loss')\n",
        "plt.plot(history_loss_2, label='BiLSTM Loss')\n",
        "plt.title('Compare Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9iZE7-wmXq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.6. Evaluating the model test**"
      ],
      "metadata": {
        "id": "YiRDDUUtnnOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_random_samples_model2(model, vocab, test_texts, test_labels, num_samples=5):\n",
        "    model.eval()\n",
        "    classes = ['Stress', 'Anxiety', 'Normal']\n",
        "\n",
        "    # Lấy index ngẫu nhiên từ tập dữ liệu test\n",
        "    random_indices = random.sample(range(len(test_texts)), num_samples)\n",
        "\n",
        "    print(f\"RATE 5 RANDOM QUESTIONS: BILSTM + ATTENTION\")\n",
        "\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        text = test_texts.iloc[idx]\n",
        "        true_label = test_labels.iloc[idx]\n",
        "\n",
        "        # Tiền xử lý văn bản\n",
        "        tokens = encode_text(text, vocab)\n",
        "        tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tensor)\n",
        "            # Dùng Softmax để lấy xác suất phần trăm\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        print(f\"\\n[{i+1}] Text: {text}\")\n",
        "        print(f\"   Predicted: {classes[pred_label]} ({probs[0][pred_label].item()*100:.2f}%) | Actual: {classes[true_label]}\")\n",
        "\n",
        "        # Đánh giá đúng/sai\n",
        "        status = \"CORRECT\" if pred_label == true_label else \"INCORRECT\"\n",
        "        print(f\"   Result: {status}\")\n",
        "\n",
        "# Chạy thử với mô hình 2\n",
        "test_random_samples_model2(model_2, vocab, test_texts, test_labels, num_samples=5)"
      ],
      "metadata": {
        "id": "_CGDI8pMncoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.3. RCNN Architecture**"
      ],
      "metadata": {
        "id": "AYPYicn1ouBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MentalHealth_RCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout):\n",
        "        super(MentalHealth_RCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=2,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout)\n",
        "\n",
        "        # Đầu vào của FC là: hidden_dim (trái) + embed_dim (từ gốc) + hidden_dim (phải)\n",
        "        self.fc_latent = nn.Linear(hidden_dim * 2 + embed_dim, hidden_dim * 2)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        # RCNN kết hợp đặc trưng cục bộ và ngữ cảnh toàn cục\n",
        "        combined = torch.cat((lstm_out, embedded), dim=2)\n",
        "        latent = torch.tanh(self.fc_latent(combined))\n",
        "        latent = latent.permute(0, 2, 1)\n",
        "\n",
        "        # Max-pooling để lấy đặc trưng mạnh nhất\n",
        "        pooled = F.max_pool1d(latent, latent.shape[2]).squeeze(2)\n",
        "        return self.fc(self.dropout(pooled))\n"
      ],
      "metadata": {
        "id": "IXRF_djTo2lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.1. Initialize and Set Up Hyperparameters**"
      ],
      "metadata": {
        "id": "17CoF47UpETB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "HIDDEN_DIM = 128\n",
        "EMBED_DIM = 100\n",
        "DROPOUT = 0.5\n",
        "VOCAB_SIZE = len(vocab)\n",
        "OUTPUT_DIM = 3\n",
        "\n",
        "model_3 = MentalHealth_RCNN(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
        "\n",
        "# Sử dụng lại weights để xử lý mất cân bằng dữ liệu\n",
        "optimizer_3 = torch.optim.Adam(model_3.parameters(), lr=1e-3)\n",
        "criterion_3 = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "\n",
        "history_loss_3 = []\n",
        "history_f1_3 = []"
      ],
      "metadata": {
        "id": "qM1q4_9Ro4sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.2. Training Loop**"
      ],
      "metadata": {
        "id": "unNfediXpeeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    model_3.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer_3.zero_grad()\n",
        "        outputs = model_3(texts)\n",
        "        loss = criterion_3(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_3.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Đánh giá sau mỗi epoch để vẽ đồ thị Learning Progress\n",
        "    val_acc, val_f1 = calculate_metrics(model_3, val_loader)\n",
        "    history_loss_3.append(total_loss/len(train_loader))\n",
        "    history_f1_3.append(val_f1)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {history_loss_3[-1]:.4f} | Val F1: {val_f1:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "xVtWBYp9pmh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.3. Generate detailed report**"
      ],
      "metadata": {
        "id": "H1THydsypqGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.eval()\n",
        "y_pred_3, y_true_3 = [], []\n",
        "\n",
        "# Đo thời gian inference\n",
        "start_inf = time.time()\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model_3(texts)\n",
        "        y_pred_3.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_3.extend(labels.numpy())\n",
        "end_inf = time.time()\n",
        "\n",
        "# Tính toán các chỉ số\n",
        "inf_time_3 = (end_inf - start_inf) / len(test_loader.dataset)\n",
        "acc_3 = accuracy_score(y_true_3, y_pred_3)\n",
        "macro_f1_3 = f1_score(y_true_3, y_pred_3, average='macro')\n",
        "weighted_f1_3 = f1_score(y_true_3, y_pred_3, average='weighted')\n",
        "\n",
        "print(f\"STATISTICAL MODEL 3: RCNN\")\n",
        "print(f\"1. Accuracy: {acc_3*100:.2f}%\")\n",
        "print(f\"2. Macro-F1: {macro_f1_3:.4f}\")\n",
        "print(f\"3. Weighted-F1: {weighted_f1_3:.4f}\")\n",
        "print(f\"4. Avg Inference Time: {inf_time_3*1000:.4f} ms/sample\")\n",
        "print(f\"\\n5. Per-class Detail:\\n\")\n",
        "print(classification_report(y_true_3, y_pred_3, target_names=['Stress', 'Anxiety', 'Normal']))"
      ],
      "metadata": {
        "id": "dwfUo-ZKpxZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.4. Confusion Matrix**"
      ],
      "metadata": {
        "id": "pOHqS7Vurlqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Tính toán ma trận nhầm lẫn từ kết quả y_true_3 và y_pred_3 đã có\n",
        "cm_3 = confusion_matrix(y_true_3, y_pred_3)\n",
        "cm_norm_3 = cm_3.astype('float') / cm_3.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_norm_3, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix (Normalized) - RCNN')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLAd5hIeri8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.5. Graph comparing learning progress**"
      ],
      "metadata": {
        "id": "zF28kwnvsAb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# So sánh Val F1-Score của cả 3\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_f1, label='KimCNN (Baseline)')\n",
        "plt.plot(history_f1_2, label='BiLSTM + Att')\n",
        "plt.plot(history_f1_3, label='RCNN (Hybrid)')\n",
        "plt.title('Compare F1-Score of 3 Models')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1')\n",
        "plt.legend()\n",
        "\n",
        "# So sánh Training Loss của cả 3\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_loss, label='KimCNN Loss')\n",
        "plt.plot(history_loss_2, label='BiLSTM Loss')\n",
        "plt.plot(history_loss_3, label='RCNN Loss')\n",
        "plt.title('Compare Loss of 3 Models')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WXxaherruZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.6. Evaluating the model test**"
      ],
      "metadata": {
        "id": "BHLgN2nWsMxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_random_rcnn(model, vocab, test_texts, test_labels, num_samples=5):\n",
        "    model.eval()\n",
        "    classes = ['Stress', 'Anxiety', 'Normal']\n",
        "    random_indices = random.sample(range(len(test_texts)), num_samples)\n",
        "\n",
        "    print(f\"EVALUATING 5 RANDOM SAMPLES: RCNN\")\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        text = test_texts.iloc[idx]\n",
        "        true_label = test_labels.iloc[idx]\n",
        "\n",
        "        tokens = encode_text(text, vocab)\n",
        "        tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tensor)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        status = \"CORRECT\" if pred_label == true_label else \"INCORRECT\"\n",
        "        print(f\"\\n[{i+1}] Text: {text}\")\n",
        "        print(f\"   Predicted: {classes[pred_label]} ({probs[0][pred_label].item()*100:.2f}%) | Actual: {classes[true_label]} | {status}\")\n",
        "\n",
        "test_random_rcnn(model_3, vocab, test_texts, test_labels)"
      ],
      "metadata": {
        "id": "9-Qunm4wsFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.4. Transformer Encoder Architecture**"
      ],
      "metadata": {
        "id": "_5weH09qu9bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MentalHealth_Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout):\n",
        "        super(MentalHealth_Transformer, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 500, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
        "        embedded = embedded + self.pos_encoder[:, :text.size(1), :]\n",
        "\n",
        "        transformer_out = self.transformer_encoder(self.dropout(embedded))\n",
        "        pooled = transformer_out.mean(dim=1)\n",
        "        return self.fc(self.dropout(pooled))\n"
      ],
      "metadata": {
        "id": "B7jrnfRCvCe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.1. Initialize and Set Up Hyperparameters**"
      ],
      "metadata": {
        "id": "6TOKnD3IvI11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 4\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 256 # Kích thước lớp ẩn Feedforward\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Truyền thêm HIDDEN_DIM vào hàm khởi tạo\n",
        "model_4 = MentalHealth_Transformer(len(vocab), EMBED_DIM, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, 3, DROPOUT).to(device)\n",
        "\n",
        "optimizer_4 = torch.optim.Adam(model_4.parameters(), lr=1e-4)\n",
        "criterion_4 = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "\n",
        "history_loss_4, history_f1_4 = [], []\n"
      ],
      "metadata": {
        "id": "-0e_yONivLVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.2. Training Loop**"
      ],
      "metadata": {
        "id": "CBkLrH59v2xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_4 = MentalHealth_Transformer(len(vocab), 100, 4, 2, 256, 3, 0.3).to(device)\n",
        "optimizer_4 = torch.optim.Adam(model_4.parameters(), lr=1e-4)\n",
        "criterion_4 = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "\n",
        "# Chạy vòng lặp huấn luyện (Training Loop)\n",
        "history_loss_4, history_f1_4 = [], []\n",
        "for epoch in range(20):\n",
        "    model_4.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer_4.zero_grad()\n",
        "        outputs = model_4(texts)\n",
        "        loss = criterion_4(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_4.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    val_acc, val_f1 = calculate_metrics(model_4, val_loader)\n",
        "    history_loss_4.append(total_loss/len(train_loader))\n",
        "    history_f1_4.append(val_f1)\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {history_loss_4[-1]:.4f} | Val F1: {val_f1:.4f}')"
      ],
      "metadata": {
        "id": "iEdCFoE5vtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.3. Export Metrics**"
      ],
      "metadata": {
        "id": "lwjXvYuHyUKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import time\n",
        "\n",
        "model_4.eval()\n",
        "y_pred_4, y_true_4 = [], []\n",
        "\n",
        "# Đo thời gian inference\n",
        "start_inf = time.time()\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model_4(texts)\n",
        "        y_pred_4.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_4.extend(labels.numpy())\n",
        "end_inf = time.time()\n",
        "\n",
        "# Tính toán các chỉ số\n",
        "inference_time_4 = (end_inf - start_inf) / len(test_loader.dataset)\n",
        "acc_4 = accuracy_score(y_true_4, y_pred_4)\n",
        "macro_f1_4 = f1_score(y_true_4, y_pred_4, average='macro')\n",
        "weighted_f1_4 = f1_score(y_true_4, y_pred_4, average='weighted')\n",
        "\n",
        "print(f\"TRANSFORMER ENCODER RESULTS\")\n",
        "print(f\"1. Accuracy: {acc_4*100:.2f}%\")\n",
        "print(f\"2. Macro-F1: {macro_f1_4:.4f}\")\n",
        "print(f\"3. Weighted-F1: {weighted_f1_4:.4f}\")\n",
        "print(f\"4. Avg Inference Time: {inference_time_4*1000:.4f} ms/sample\")\n",
        "print(f\"\\n5. Per-class Detail:\\n\")\n",
        "print(classification_report(y_true_4, y_pred_4, target_names=['Stress', 'Anxiety', 'Normal']))"
      ],
      "metadata": {
        "id": "Cq_H11lzyGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.4. Confusion Matrix**"
      ],
      "metadata": {
        "id": "PUyRcujCyb9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm_4 = confusion_matrix(y_true_4, y_pred_4)\n",
        "cm_norm_4 = cm_4.astype('float') / cm_4.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_norm_4, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix (Normalized) - Transformer')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DC7eaNEAyZnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.5. Learning Progress**"
      ],
      "metadata": {
        "id": "sUA2-hdDyqeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# So sánh F1-Score\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_f1, label='KimCNN')\n",
        "plt.plot(history_f1_2, label='BiLSTM + Att')\n",
        "plt.plot(history_f1_3, label='RCNN')\n",
        "plt.plot(history_f1_4, label='Transformer (Ours)')\n",
        "plt.title('Compare F1-Score of 4 Models')\n",
        "plt.legend()\n",
        "\n",
        "# So sánh Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_loss, label='KimCNN')\n",
        "plt.plot(history_loss_2, label='BiLSTM')\n",
        "plt.plot(history_loss_3, label='RCNN')\n",
        "plt.plot(history_loss_4, label='Transformer')\n",
        "plt.title('Compare Loss of 4 Models')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmTgNnv2yidi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_random_transformer(model, vocab, test_texts, test_labels, num_samples=5):\n",
        "    model.eval()\n",
        "    classes = ['Stress', 'Anxiety', 'Normal']\n",
        "\n",
        "    # Lấy index ngẫu nhiên từ tập dữ liệu test\n",
        "    random_indices = random.sample(range(len(test_texts)), num_samples)\n",
        "\n",
        "    print(f\" EVALUATING 5 RANDOM SAMPLES: TRANSFORMER ENCODER \")\n",
        "\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        text = test_texts.iloc[idx]\n",
        "        true_label = test_labels.iloc[idx]\n",
        "\n",
        "        # Tiền xử lý văn bản\n",
        "        tokens = encode_text(text, vocab)\n",
        "        tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tensor)\n",
        "            # Lấy xác suất % để xem độ tự tin của Transformer\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        status = \"CORRECT\" if pred_label == true_label else \"INCORRECT\"\n",
        "        print(f\"\\n[{i+1}] Text: {text}\")\n",
        "        print(f\"   Predicted: {classes[pred_label]} ({probs[0][pred_label].item()*100:.2f}%) | Actual: {classes[true_label]} | {status}\")\n",
        "\n",
        "# Thực thi dự đoán\n",
        "test_random_transformer(model_4, vocab, test_texts, test_labels)"
      ],
      "metadata": {
        "id": "x1V9o9L4y9jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.5. PhoBERT + Custom Head.**"
      ],
      "metadata": {
        "id": "X2rFiiyUzEpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "op0bfwQB2C6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.1. PhoBERT Architecture + Custom Head**"
      ],
      "metadata": {
        "id": "_6YIJXvF2LY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn as nn\n",
        "\n",
        "class MentalHealth_PhoBERT_Custom(nn.Module):\n",
        "    def __init__(self, model_name, output_dim, dropout_rate):\n",
        "        super(MentalHealth_PhoBERT_Custom, self).__init__()\n",
        "        # 1. Tải PhoBERT pre-trained\n",
        "        self.phobert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # 2. Custom Head: Attentive Pooling\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(768, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # 3. Multi-sample Dropout để tăng tính ổn định\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(dropout_rate) for _ in range(5)])\n",
        "\n",
        "        self.fc = nn.Linear(768, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Lấy output từ PhoBERT\n",
        "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state # [batch, seq_len, 768]\n",
        "\n",
        "        # Tính toán Attention weights\n",
        "        weights = self.attention(last_hidden_state)\n",
        "        context_vector = torch.sum(weights * last_hidden_state, dim=1)\n",
        "\n",
        "        # Multi-sample Dropout: Trung bình cộng kết quả từ nhiều lần dropout\n",
        "        logits = sum([self.fc(drop(context_vector)) for drop in self.dropouts]) / 5\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "_JcJ157_2RBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2. Initialize Tokenizer and DataLoader**"
      ],
      "metadata": {
        "id": "I-OJ4Ggg2YSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "def tokenize_for_phobert(texts):\n",
        "    return tokenizer(texts.tolist(), padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize dữ liệu\n",
        "train_encodings = tokenize_for_phobert(train_texts)\n",
        "val_encodings = tokenize_for_phobert(val_texts)\n",
        "test_encodings = tokenize_for_phobert(test_texts)\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model_5 = MentalHealth_PhoBERT_Custom(\"vinai/phobert-base\", 3, 0.2).to(device)\n",
        "optimizer_5 = torch.optim.AdamW(model_5.parameters(), lr=2e-5) # PhoBERT cần lr cực nhỏ"
      ],
      "metadata": {
        "id": "36ZhHUvA2fQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def create_phobert_loader(encodings, labels, batch_size=16):\n",
        "    dataset = TensorDataset(\n",
        "        encodings['input_ids'],\n",
        "        encodings['attention_mask'],\n",
        "        torch.tensor(labels.values)\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_loader_5 = create_phobert_loader(train_encodings, train_labels)\n",
        "val_loader_5 = create_phobert_loader(val_encodings, val_labels)\n",
        "test_loader_5 = create_phobert_loader(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "ftscCruD2xMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.3. Training Loop**"
      ],
      "metadata": {
        "id": "k5hOC57421Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_loss_5, history_f1_5 = [], []\n",
        "\n",
        "for epoch in range(20):\n",
        "    model_5.train()\n",
        "    total_loss = 0\n",
        "    for ids, mask, labels in train_loader_5:\n",
        "        ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_5.zero_grad()\n",
        "        outputs = model_5(ids, mask)\n",
        "        loss = criterion_4(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_5.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Đánh giá Val F1\n",
        "    model_5.eval()\n",
        "    y_val_p, y_val_t = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, mask, labels in val_loader_5:\n",
        "            ids, mask = ids.to(device), mask.to(device)\n",
        "            outputs = model_5(ids, mask)\n",
        "            y_val_p.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            y_val_t.extend(labels.numpy())\n",
        "\n",
        "    val_f1 = f1_score(y_val_t, y_val_p, average='macro')\n",
        "    history_loss_5.append(total_loss/len(train_loader_5))\n",
        "    history_f1_5.append(val_f1)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {history_loss_5[-1]:.4f} | Val F1: {val_f1:.4f}')"
      ],
      "metadata": {
        "id": "_EaLCrcE26Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.4. Confusion Matrix**"
      ],
      "metadata": {
        "id": "7aD_6cdN67Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.eval()\n",
        "y_pred_5, y_true_5 = [], []\n",
        "with torch.no_grad():\n",
        "    for ids, mask, labels in test_loader_5:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        outputs = model_5(ids, mask)\n",
        "        y_pred_5.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_5.extend(labels.numpy())\n",
        "\n",
        "cm_5 = confusion_matrix(y_true_5, y_pred_5)\n",
        "cm_norm_5 = cm_5.astype('float') / cm_5.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_norm_5, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "            yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "plt.title('Confusion Matrix (Normalized) - PhoBERT + Custom Head')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T4KRJ3gi64II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# So sánh F1-Score của cả 5 mô hình\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_f1, label='KimCNN')\n",
        "plt.plot(history_f1_2, label='BiLSTM + Att')\n",
        "plt.plot(history_f1_3, label='RCNN')\n",
        "plt.plot(history_f1_4, label='Transformer (Ours)')\n",
        "plt.plot(history_f1_5, label='PhoBERT (SOTA)', linewidth=3, linestyle='--')\n",
        "plt.title('Compare F1-Score of 5 Models')\n",
        "plt.legend()\n",
        "\n",
        "# So sánh Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_loss, label='KimCNN')\n",
        "plt.plot(history_loss_2, label='BiLSTM')\n",
        "plt.plot(history_loss_3, label='RCNN')\n",
        "plt.plot(history_loss_4, label='Transformer')\n",
        "plt.plot(history_loss_5, label='PhoBERT', linewidth=3, linestyle='--')\n",
        "plt.title('Compare Loss of 5 Models')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cnZ86yKZ6_Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "model_5.eval()\n",
        "y_pred_5, y_true_5 = [], []\n",
        "\n",
        "# Đo thời gian inference\n",
        "start_inf = time.time()\n",
        "with torch.no_grad():\n",
        "    for ids, mask, labels in test_loader_5:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        outputs = model_5(ids, mask)\n",
        "        y_pred_5.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_5.extend(labels.numpy())\n",
        "end_inf = time.time()\n",
        "\n",
        "# Tính toán các chỉ số bắt buộc\n",
        "inf_time_5 = (end_inf - start_inf) / len(test_loader_5.dataset)\n",
        "acc_5 = accuracy_score(y_true_5, y_pred_5)\n",
        "macro_f1_5 = f1_score(y_true_5, y_pred_5, average='macro')\n",
        "weighted_f1_5 = f1_score(y_true_5, y_pred_5, average='weighted')\n",
        "\n",
        "print(f\"MODEL 5 STATISTICS: PHOBERT + CUSTOM HEAD\")\n",
        "print(f\"1. Accuracy: {acc_5*100:.2f}%\")\n",
        "print(f\"2. Macro-F1: {macro_f1_4:.4f}\")\n",
        "print(f\"3. Weighted-F1: {weighted_f1_5:.4f}\")\n",
        "print(f\"4. Avg Inference Time: {inf_time_5*1000:.4f} ms/sample\")\n",
        "print(f\"\\n5. DETAILED REPORT:\\n\")\n",
        "print(classification_report(y_true_5, y_pred_5, target_names=['Stress', 'Anxiety', 'Normal']))"
      ],
      "metadata": {
        "id": "3QdF0A6V7sAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices = random.sample(range(len(test_texts)), 5)\n",
        "print(f\"RANDOM EVALUATION: PHOBERT\")\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "    text = test_texts.iloc[idx]\n",
        "    true_label = test_labels.iloc[idx]\n",
        "\n",
        "    enc = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_5(enc['input_ids'], enc['attention_mask'])\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    status = \"CORRECT\" if pred_label == true_label else \"INCORRECT\"\n",
        "    print(f\"\\n[{i+1}] Text: {text}\")\n",
        "    print(f\"   Predicted: {['Stress', 'Anxiety', 'Normal'][pred_label]} ({probs[0][pred_label].item()*100:.2f}%) | Result: {status}\")"
      ],
      "metadata": {
        "id": "__MUeohY7dzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 3: Using the PhoBERT model + Custom Head to experiment with public data**\n",
        "##**Public Benchmark (UIT-VSMEC):** Using the tridm/UIT-VSMEC dataset from HuggingFace as the public control set."
      ],
      "metadata": {
        "id": "M96MCA0w9VwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Tải tập Public Benchmark\n",
        "dataset_pub = load_dataset(\"tridm/UIT-VSMEC\")"
      ],
      "metadata": {
        "id": "MWMDCCa_9awP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.1. STABILITY**"
      ],
      "metadata": {
        "id": "G9CO2Jcg_xZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "seeds = [42, 123, 2024]\n",
        "stability_f1_scores = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f\"CURRENTLY RETRAINING WITH SEED: {s}\")\n",
        "    torch.manual_seed(s)\n",
        "    np.random.seed(s)\n",
        "\n",
        "    # Khởi tạo mới hoàn toàn\n",
        "    model_st = MentalHealth_PhoBERT_Custom(\"vinai/phobert-base\", 3, 0.2).to(device)\n",
        "    optimizer_st = torch.optim.AdamW(model_st.parameters(), lr=2e-5)\n",
        "\n",
        "    for epoch in range(5): # Chạy 5 epoch để kiểm tra độ ổn định nhanh\n",
        "        model_st.train()\n",
        "        for ids, mask, labels in train_loader_5:\n",
        "            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
        "            optimizer_st.zero_grad()\n",
        "            outputs = model_st(ids, mask)\n",
        "            loss = criterion_4(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_st.step()\n",
        "\n",
        "    # Sau khi train xong mới đánh giá\n",
        "    model_st.eval()\n",
        "    y_p, y_t = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, mask, labels in test_loader_5:\n",
        "            outputs = model_st(ids.to(device), mask.to(device))\n",
        "            y_p.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            y_t.extend(labels.numpy())\n",
        "\n",
        "    current_f1 = f1_score(y_t, y_p, average='macro')\n",
        "    stability_f1_scores.append(current_f1)\n",
        "    print(f\"Seed {s} complete. Macro-F1: {current_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n=> REAL STABILITY RESULTS: {np.mean(stability_f1_scores):.4f} ± {np.std(stability_f1_scores):.4f}\")"
      ],
      "metadata": {
        "id": "6tHbzZMW_uVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.2. Public Benchmark**"
      ],
      "metadata": {
        "id": "u2hWBAyfC3jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def mapping_to_3_classes(example):\n",
        "    v_label = example['Emotion']\n",
        "    if v_label in [2, 4, 5]:\n",
        "        new_label = 0\n",
        "    elif v_label == 3:\n",
        "        new_label = 1\n",
        "    else:\n",
        "        new_label = 2\n",
        "    return {'text': str(example['Sentence']), 'label': int(new_label)}\n",
        "\n",
        "print(\"PREPARING PUBLIC BENCHMARK DATA\")\n",
        "# Thực hiện map và chuyển đổi sang list Python\n",
        "mapped_vsmec = dataset_pub.map(mapping_to_3_classes, remove_columns=dataset_pub['train'].column_names)\n",
        "\n",
        "def create_final_loader(split_name):\n",
        "\n",
        "    texts = list(mapped_vsmec[split_name]['text'])\n",
        "    labels = list(mapped_vsmec[split_name]['label'])\n",
        "\n",
        "    encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels))\n",
        "    return DataLoader(dataset, batch_size=16, shuffle=(split_name == 'train'))\n",
        "\n",
        "# Tạo loader\n",
        "pub_train_loader = create_final_loader('train')\n",
        "pub_test_loader = create_final_loader('test')\n",
        "\n",
        "# 2. Khởi tạo mô hình PhoBERT (Public)\n",
        "print(\"INITIALIZING PHOBERT MODEL (PUBLIC)\")\n",
        "model_pub = MentalHealth_PhoBERT_Custom(\"vinai/phobert-base\", 3, 0.2).to(device)\n",
        "optimizer_pub = torch.optim.AdamW(model_pub.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Huấn luyện (Train) - 5 Epochs\n",
        "print(\"\\nTRAINING PHOBERT MODEL (PUBLIC)\")\n",
        "for epoch in range(5):\n",
        "    model_pub.train()\n",
        "    total_loss = 0\n",
        "    for ids, mask, labels in tqdm(pub_train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
        "        optimizer_pub.zero_grad()\n",
        "        outputs = model_pub(ids, mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_pub.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(pub_train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "mUV4aJDiC7mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"\\nPUBLIC BENCHMARK RESULTS\")\n",
        "model_pub.eval()\n",
        "y_pred_pub, y_true_pub = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for ids, mask, labels in pub_test_loader:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        outputs = model_pub(ids, mask)\n",
        "        y_pred_pub.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_pub.extend(labels.numpy())\n",
        "\n",
        "print(classification_report(\n",
        "    y_true_pub,\n",
        "    y_pred_pub,\n",
        "    labels=[0, 1, 2],\n",
        "    target_names=['Stress', 'Anxiety', 'Normal']\n",
        "))"
      ],
      "metadata": {
        "id": "t6zQdYlXKI7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.3. CROSS-DOMAIN**"
      ],
      "metadata": {
        "id": "8m_Hv-wAg6qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1. Định nghĩa hàm vẽ ma trận nhầm lẫn\n",
        "def plot_normalized_cm(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
        "    # Chuẩn hóa để tính tỷ lệ % trên từng dòng\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm_norm = np.nan_to_num(cm_norm) # Tránh lỗi chia cho 0 nếu nhãn đó không có mẫu nào\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Purples',\n",
        "                xticklabels=['Stress', 'Anxiety', 'Normal'],\n",
        "                yticklabels=['Stress', 'Anxiety', 'Normal'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# 2. Thực hiện đánh giá Cross-domain\n",
        "model_5.eval() # Sử dụng mô hình tốt nhất từ dữ liệu tự thu\n",
        "y_pred_cross, y_true_cross = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Kiểm thử trên tập Public (Target Domain)\n",
        "    for ids, mask, labels in pub_test_loader:\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        outputs = model_5(ids, mask)\n",
        "        y_pred_cross.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        y_true_cross.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nDetailed Report:\")\n",
        "print(classification_report(\n",
        "    y_true_cross,\n",
        "    y_pred_cross,\n",
        "    labels=[0, 1, 2],\n",
        "    target_names=['Stress', 'Anxiety', 'Normal'],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# 4. Vẽ ma trận nhầm lẫn\n",
        "plot_normalized_cm(y_true_cross, y_pred_cross, \"Confusion Matrix: Cross-domain (PhoBERT)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XlouRf4Wg-yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.4 Performance graphs for each label against PhoBERT**"
      ],
      "metadata": {
        "id": "nJh-kHfXx3EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "labels = ['Stress', 'Anxiety', 'Normal']\n",
        "precision = [0.78, 0.76, 0.85]\n",
        "recall = [0.75, 0.74, 0.88]\n",
        "f1 = [0.76, 0.75, 0.86]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Vẽ 3 cột cho mỗi nhóm nhãn\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision', color='#add8e6', edgecolor='black')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall', color='#87cefa', edgecolor='black')\n",
        "rects3 = ax.bar(x + width, f1, width, label='F1-Score', color='#4169e1', edgecolor='black')\n",
        "\n",
        "ax.set_ylabel('Score (0.0 - 1.0)', fontsize=12)\n",
        "ax.set_title('Hiệu năng chi tiết của mô hình PhoBERT theo từng lớp nhãn', fontsize=15, pad=20)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=12, fontweight='bold')\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_ylim(0, 1.1)\n",
        "\n",
        "\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HmIa6gYgHG96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\n--- Estimated Parameters (Trainable) ---\")\n",
        "print(f\"KimCNN: {count_parameters(model):,}\")\n",
        "print(f\"BiLSTM + Attention: {count_parameters(model_2):,}\")\n",
        "print(f\"RCNN: {count_parameters(model_3):,}\")\n",
        "print(f\"Transformer Encoder: {count_parameters(model_4):,}\")\n",
        "print(f\"PhoBERT + Custom Head: {count_parameters(model_5):,}\")"
      ],
      "metadata": {
        "id": "Ljd9khoP23Wq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}